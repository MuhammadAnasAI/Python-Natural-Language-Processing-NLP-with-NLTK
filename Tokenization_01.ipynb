{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be7a829",
   "metadata": {},
   "source": [
    "# Tokenization, Stemming and Lemmatization in the NLP using NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc6cc6",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "- `Tokenization` is the process of converting the paragraph or sentence into the number of `Tokens` or `words` which uses the `string` or `list` of numbers.\n",
    "- All these words are into the form of number list. Each list represent the each number of words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51799a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the libraries:\n",
    "import nltk \n",
    "nltk.download()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180529ed",
   "metadata": {},
   "source": [
    "- Take a `paragraph` to be a string of characters that may contain spaces, punctuation, and capital letters. \n",
    "- Write a `paragraph ` function that takes a `paragraph` as input and returns the number of words in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9da6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Artificial Intelligence is a branch of computer science focused on creating machines or software that can perform tasks traditionally requiring human intelligence. These tasks include learning from experience, reasoning, recognizing patterns, understanding natural language, and making decisions. Unlike traditional software that follows explicit rules programmed by humans, AI systems are designed to perceive data, learn from it, and adapt their behavior autonomously. This ability makes AI a fascinating and powerful tool shaping the future.\n",
    "The concept of AI was first formally proposed in the 1950s, with pioneers like Alan Turing and John von Neumann laying the groundwork. Alan Turing's question, \"Can machines think?\" and his formulation of the Turing Test became foundational ideas. Since then, AI research has progressed from simple rule-based systems to complex neural networks inspired by the human brain.\n",
    "Machine Learning enables computers to learn from data without explicit programming for every task. It relies on mathematical models including probability, statistics, linear algebra, and optimization theory. Algorithms optimize a cost or loss function by adjusting parameters to minimize the error in predictions. Techniques like supervised learning (learning from labeled data), unsupervised learning (finding patterns in unlabeled data), and reinforcement learning (learning through rewards and punishments) are fundamental.\n",
    "Deep Learning, a subset of machine learning, uses artificial neural networks with many layers to model increasingly abstract features from data. The core mathematical operation in neural networks is the weighted sum followed by a nonlinear activation function, enabling the model to approximate complex functions. Training deep networks requires backpropagation, a method using calculus and chain rule to compute gradients of the loss with respect to all weights, enabling efficient optimization via gradient descent.\n",
    "Natural Language Processing uses statistical and neural models to enable computers to understand, interpret, and generate human language. Techniques involve sequence modeling with recurrent neural networks (RNNs), transformers, and attention mechanisms.\n",
    "Computer Vision enables machines to interpret visual data through convolutional neural networks (CNNs), which apply convolutional filters to detect edges, textures, and objects.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28bb7e",
   "metadata": {},
   "source": [
    "- To convert the paragraph into the `sentence` format, we need to split the paragraph into sentences. We can use the `nltk` library `sent_tokenize` function to achieve this.\n",
    "- `nltk.sent_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77419a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of the paragraph into sentences:\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4e26d",
   "metadata": {},
   "source": [
    "- To convert the `paragraph` in to `words`, we use the `nltk` library's `word_tokenize()` function. This function splits the text paragraph into individual words.\n",
    "- `nltk.word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3802da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb0099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence is a branch of computer science focused on creating machines or software that can perform tasks traditionally requiring human intelligence.\n",
      "These tasks include learning from experience, reasoning, recognizing patterns, understanding natural language, and making decisions.\n",
      "Unlike traditional software that follows explicit rules programmed by humans, AI systems are designed to perceive data, learn from it, and adapt their behavior autonomously.\n",
      "This ability makes AI a fascinating and powerful tool shaping the future.\n",
      "The concept of AI was first formally proposed in the 1950s, with pioneers like Alan Turing and John von Neumann laying the groundwork.\n",
      "Alan Turing's question, \"Can machines think?\"\n",
      "and his formulation of the Turing Test became foundational ideas.\n",
      "Since then, AI research has progressed from simple rule-based systems to complex neural networks inspired by the human brain.\n",
      "Machine Learning enables computers to learn from data without explicit programming for every task.\n",
      "It relies on mathematical models including probability, statistics, linear algebra, and optimization theory.\n",
      "Algorithms optimize a cost or loss function by adjusting parameters to minimize the error in predictions.\n",
      "Techniques like supervised learning (learning from labeled data), unsupervised learning (finding patterns in unlabeled data), and reinforcement learning (learning through rewards and punishments) are fundamental.\n",
      "Deep Learning, a subset of machine learning, uses artificial neural networks with many layers to model increasingly abstract features from data.\n",
      "The core mathematical operation in neural networks is the weighted sum followed by a nonlinear activation function, enabling the model to approximate complex functions.\n",
      "Training deep networks requires backpropagation, a method using calculus and chain rule to compute gradients of the loss with respect to all weights, enabling efficient optimization via gradient descent.\n",
      "Natural Language Processing uses statistical and neural models to enable computers to understand, interpret, and generate human language.\n",
      "Techniques involve sequence modeling with recurrent neural networks (RNNs), transformers, and attention mechanisms.\n",
      "Computer Vision enables machines to interpret visual data through convolutional neural networks (CNNs), which apply convolutional filters to detect edges, textures, and objects.\n"
     ]
    }
   ],
   "source": [
    "# print the all the sentences one by one:\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "023120e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the words are:  ['Artificial', 'Intelligence', 'is', 'a', 'branch', 'of', 'computer', 'science', 'focused', 'on', 'creating', 'machines', 'or', 'software', 'that', 'can', 'perform', 'tasks', 'traditionally', 'requiring', 'human', 'intelligence', '.', 'These', 'tasks', 'include', 'learning', 'from', 'experience', ',', 'reasoning', ',', 'recognizing', 'patterns', ',', 'understanding', 'natural', 'language', ',', 'and', 'making', 'decisions', '.', 'Unlike', 'traditional', 'software', 'that', 'follows', 'explicit', 'rules', 'programmed', 'by', 'humans', ',', 'AI', 'systems', 'are', 'designed', 'to', 'perceive', 'data', ',', 'learn', 'from', 'it', ',', 'and', 'adapt', 'their', 'behavior', 'autonomously', '.', 'This', 'ability', 'makes', 'AI', 'a', 'fascinating', 'and', 'powerful', 'tool', 'shaping', 'the', 'future', '.', 'The', 'concept', 'of', 'AI', 'was', 'first', 'formally', 'proposed', 'in', 'the', '1950s', ',', 'with', 'pioneers', 'like', 'Alan', 'Turing', 'and', 'John', 'von', 'Neumann', 'laying', 'the', 'groundwork', '.', 'Alan', 'Turing', \"'s\", 'question', ',', '``', 'Can', 'machines', 'think', '?', \"''\", 'and', 'his', 'formulation', 'of', 'the', 'Turing', 'Test', 'became', 'foundational', 'ideas', '.', 'Since', 'then', ',', 'AI', 'research', 'has', 'progressed', 'from', 'simple', 'rule-based', 'systems', 'to', 'complex', 'neural', 'networks', 'inspired', 'by', 'the', 'human', 'brain', '.', 'Machine', 'Learning', 'enables', 'computers', 'to', 'learn', 'from', 'data', 'without', 'explicit', 'programming', 'for', 'every', 'task', '.', 'It', 'relies', 'on', 'mathematical', 'models', 'including', 'probability', ',', 'statistics', ',', 'linear', 'algebra', ',', 'and', 'optimization', 'theory', '.', 'Algorithms', 'optimize', 'a', 'cost', 'or', 'loss', 'function', 'by', 'adjusting', 'parameters', 'to', 'minimize', 'the', 'error', 'in', 'predictions', '.', 'Techniques', 'like', 'supervised', 'learning', '(', 'learning', 'from', 'labeled', 'data', ')', ',', 'unsupervised', 'learning', '(', 'finding', 'patterns', 'in', 'unlabeled', 'data', ')', ',', 'and', 'reinforcement', 'learning', '(', 'learning', 'through', 'rewards', 'and', 'punishments', ')', 'are', 'fundamental', '.', 'Deep', 'Learning', ',', 'a', 'subset', 'of', 'machine', 'learning', ',', 'uses', 'artificial', 'neural', 'networks', 'with', 'many', 'layers', 'to', 'model', 'increasingly', 'abstract', 'features', 'from', 'data', '.', 'The', 'core', 'mathematical', 'operation', 'in', 'neural', 'networks', 'is', 'the', 'weighted', 'sum', 'followed', 'by', 'a', 'nonlinear', 'activation', 'function', ',', 'enabling', 'the', 'model', 'to', 'approximate', 'complex', 'functions', '.', 'Training', 'deep', 'networks', 'requires', 'backpropagation', ',', 'a', 'method', 'using', 'calculus', 'and', 'chain', 'rule', 'to', 'compute', 'gradients', 'of', 'the', 'loss', 'with', 'respect', 'to', 'all', 'weights', ',', 'enabling', 'efficient', 'optimization', 'via', 'gradient', 'descent', '.', 'Natural', 'Language', 'Processing', 'uses', 'statistical', 'and', 'neural', 'models', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Techniques', 'involve', 'sequence', 'modeling', 'with', 'recurrent', 'neural', 'networks', '(', 'RNNs', ')', ',', 'transformers', ',', 'and', 'attention', 'mechanisms', '.', 'Computer', 'Vision', 'enables', 'machines', 'to', 'interpret', 'visual', 'data', 'through', 'convolutional', 'neural', 'networks', '(', 'CNNs', ')', ',', 'which', 'apply', 'convolutional', 'filters', 'to', 'detect', 'edges', ',', 'textures', ',', 'and', 'objects', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print all the Words:\n",
    "print(\"All the words are: \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab281961",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "- Stemming is a process of reducing words to their base or root form. It is a simple and fast method of reducing words to their base form.\n",
    "- Porter Stemmer is a popular stemming algorithm that is widely used in natural language processing tasks.\n",
    "- NLTK library in Python has a built-in Porter Stemmer that can be used to stem words.\n",
    "- For example, the word \"running\" can be stemmed to \"run\" using Porter Stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d96006",
   "metadata": {},
   "source": [
    "### Stopwords:\n",
    "\n",
    "- `stopwords`: a list of words that are ignored in the analysis. These words are common and do not add much value to the analysis. They can be things like \"the\", \"and\", \"a\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5c2a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PosterStemmer:\n",
    "from nltk.stem import PorterStemmer\n",
    "# Import the Stopwords:\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a889136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"My name is Muhammad Anas. Graduated in Artificial Intelligence from IUB. My experties in Machine learning and Deep learning with python.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b01c9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eae4fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the stemming algorithm to the text:\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(len(sent)):\n",
    "    words = nltk.word_tokenize(sent[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sent[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e2ddb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name muhammad ana .\n",
      "graduat artifici intellig iub .\n",
      "my experti machin learn deep learn python .\n"
     ]
    }
   ],
   "source": [
    "# Print the stemmer:\n",
    "for sen in sent:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13355d2a",
   "metadata": {},
   "source": [
    "# Lemmetization\n",
    "In **Natural Language Processing (NLP)**, **lemmatization** is the process of reducing a word to its **base or dictionary form** (called the *lemma*), while ensuring that the resulting form is a valid word in the language.\n",
    "\n",
    "Unlike **stemming**, which just chops off word endings based on simple rules, lemmatization uses **linguistic knowledge** (like vocabulary and morphological analysis) to consider the word’s **part of speech** and produce a meaningful root form.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* *running* → **run**\n",
    "* *better* → **good**\n",
    "* *mice* → **mouse**\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* Requires understanding of **context** (e.g., *\"saw\"* could be the verb *see* or the noun *saw* — the meaning changes).\n",
    "* Often uses a **lexicon** or **WordNet** for reference.\n",
    "* More accurate than stemming but computationally heavier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e5eaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lemmetization library:\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2ba8a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Data Analysis is the process of examining data sets to draw conclusions about the information they contain.Python libraries like seaborn, matplotlib and plotly mostly used for this purpose.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c6ed924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text:\n",
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "918e9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Lemmetization process:\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply the for loop on the tokens:\n",
    "for i in range(len(tokens)):\n",
    "    words = nltk.word_tokenize(tokens[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    tokens[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0adda0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "Analysis\n",
      "\n",
      "\n",
      "process\n",
      "\n",
      "examining\n",
      "data\n",
      "set\n",
      "\n",
      "draw\n",
      "conclusion\n",
      "\n",
      "\n",
      "information\n",
      "\n",
      "contain.Python\n",
      "library\n",
      "like\n",
      "seaborn\n",
      ",\n",
      "matplotlib\n",
      "\n",
      "plotly\n",
      "mostly\n",
      "used\n",
      "\n",
      "\n",
      "purpose\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47853e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
